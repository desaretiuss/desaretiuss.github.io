---
layout: post
title: Machine Learning
---

Last months I've started to introduce myself a bit with machine learning.
It's not a present goal for me to switch to an ML engineering role, and I also believe there is a lot
of hype going around regarding generative AI etc.
Still I think it's very important to have an understanding (however superficial) of such a powerful 
area of Computer Science.

I started with a quick primer on Probability and Statistics covering distributions, covariance, correlation, 
Bayesian approach etc.

Then I read on some Information Theory topics. I recalled a one-semester course we had during university, were 
we covered Shannon, encodings and such, but more from a circuits point of view as far as I remember.
So the concepts of rare events, entropy, divergence, information gain were new to me and quite interesting.

Afterwards: Classification process. Classes, Clustering, Boundary methods, The Course of Dimensionality, etc.

![The course of dimensionality](../images/dimensionality.png){:height="339px" width="521px"}

Overfitting: You focus too much on the details during training and fail to see the big picture on real data.
Underfitting: You try hard to see the big picture, to make generalizations, but you miss a lot of details on real world data.

Then some tips on the process of Data Preparation:

How to shrink a dataset: 
    Remove redundant, irrelevant features
    Combine features (Dimensionality Reduction)
        We can use PCA (Princial Component Analysis), a mathematical technique for that purpose

Data augmentation:
    Let's say our training dataset is quite small, that's all the data we could get our hands to.
    There are data augmentation techniques to artificially expand both the size and diversity of a dataset.
    Geometric transformations, noise injection, mixing data, and alike.





a lot of techniques are already part of libaries
Vector databases

